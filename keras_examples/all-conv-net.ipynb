{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dropout, Dense, Activation, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import AveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "# 10 classes for cifar10.\n",
    "nb_classes = 10\n",
    "\n",
    "# Download and load the data.\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert to float datatype.\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "# Normalize so the max value is 1.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Change to 1-hot encoding.\n",
    "Y_train = keras.utils.np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = keras.utils.np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Check our shapes.\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subtract by the mean pixel, so image data is centered around 0.\n",
    "# This differs from the paper in which, \n",
    "# \"images were whitened and contrast normalized following Goodfellow et al. (2013).\"\n",
    "mean_px = X_train.mean(axis=0).mean(axis=0).mean(axis=0)\n",
    "X_train -= mean_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv(x, nb_filters, kernel_size=(3,3), strides=(1,1), padding='same', \n",
    "         kernel_initializer=initializers.he_uniform(),  # Not sure how weights were initialized.\n",
    "         kernel_regularizer=regularizers.l2(0.001),  # \"all models were regularized with weight decay \\lambda=0.001\"\n",
    "         activation='relu',  # Table 1,2    \n",
    "        ):\n",
    "    \"\"\"Convolutional layer\"\"\"\n",
    "\n",
    "    x = Conv2D(nb_filters, kernel_size=kernel_size, strides=strides, \n",
    "           padding=padding, kernel_initializer=kernel_initializer, \n",
    "           kernel_regularizer=kernel_regularizer, activation=activation)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model C from Table 1 and 2.\n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "x = Dropout(0.2)(inputs) # \"dropout probabilities were 20% for dropping out inputs ...\"\n",
    "\n",
    "x = conv(x, 96)\n",
    "x = conv(x, 96)\n",
    "x = conv(x, 96, strides=(2,2))\n",
    "x = Dropout(0.5)(x)  # \"dropout ... after the layer replacing the pooling layer ... and 50% ...\"\n",
    "\n",
    "x = conv(x, 192)\n",
    "x = conv(x, 192)\n",
    "x = conv(x, 192, strides=(2,2))\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = conv(x, 192)\n",
    "x = conv(x, 192, kernel_size=(1,1), padding='valid')\n",
    "x = conv(x, nb_filters=nb_classes, kernel_size=(1,1), padding='valid')\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "preds = Activation('softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=preds)\n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 96)        2688      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 96)        83040     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 96)        83040     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 192)       166080    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 192)       331968    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 192)         331968    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 192)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 192)         331968    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 192)         37056     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 10)          1930      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,369,738.0\n",
      "Trainable params: 1,369,738.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"all-conv-weights.hdf5\", verbose=1, save_best_only=True)\n",
    "tb = TensorBoard(log_dir='/local-scratch/jer/projects/ex_keras/logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "model.fit(X_train[:500], Y_train[:500], epochs=5, validation_split=0.2, shuffle=True, verbose=2, callbacks=[checkpointer, tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
